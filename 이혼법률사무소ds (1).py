# -*- coding: utf-8 -*-
"""이혼법률사무소DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQyFEs1USBVM2cMxYJ-569f8Mkq7AXa4

# **애매모호한 변수 제거, 부정 -> 긍정으로 바꾼 엑셀 파일**
- 11/10 (월)
"""

# ==========================================================
# Divorce.xlsx - 일부 부정문만 척도 반전 + 일부 변수 삭제
# ==========================================================

import pandas as pd
from google.colab import files

# 1️⃣ divorce.xlsx 업로드
print("📂 'divorce.xlsx' 파일을 업로드하세요.")
uploaded = files.upload()  # 업로드 창이 뜸

# 2️⃣ 파일 불러오기
df = pd.read_excel('/content/divorce.xlsx')
print("✅ 파일 불러오기 완료:", df.shape)

# 3️⃣ 역코딩(척도 반전)할 문항 번호
reverse_items = [6,7,31,32,33,34,36,37,39,40,41,42]

# 4️⃣ 삭제할 문항 번호
delete_items = [4,35,43,44,45,47,50,51,52,53,54]

# 5️⃣ 실제 컬럼명으로 변환 (Atr1~Atr54 형태)
reverse_cols = [f"Atr{i}" for i in reverse_items if f"Atr{i}" in df.columns]
delete_cols = [f"Atr{i}" for i in delete_items if f"Atr{i}" in df.columns]

print("\n🔁 역코딩 대상:", reverse_cols)
print("🗑️ 삭제 대상:", delete_cols)

# 6️⃣ 역코딩 실행 (0~4 척도 → 4 - X)
df[reverse_cols] = 4 - df[reverse_cols]

# 7️⃣ 불필요한 열 삭제
df = df.drop(columns=delete_cols, errors='ignore')

# 8️⃣ 결과 저장
output_file = "divorce_revised_selected.xlsx"
df.to_excel(output_file, index=False)

print(f"\n✅ 새 파일 생성 완료: {output_file}")
print("📥 Colab 왼쪽 '파일' 탭에서 다운로드할 수 있습니다.")

"""# 변수 중요도
- 수정한 엑셀 파일의 변수 중요도 출력
- 11/10 (월)
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# 1. 데이터 불러오기 (Load data)
df = pd.read_excel("divorce_revised_selected.xlsx")

# 2. 입력(X)과 출력(y) 구분 (Separate X and y)
y = df['Class']
X = df.drop(columns=['Class'])

# 3. 랜덤 포레스트 분류 모델 학습 (Train Random Forest Classifier Model)
rf = RandomForestClassifier(n_estimators=128, random_state=42)
rf.fit(X, y)

# 4. 변수 중요도 계산 (MDI 방식) (Calculate Feature Importance using MDI)
importances = pd.Series(rf.feature_importances_, index=X.columns)

# 5. 변수 중요도 시각화 (Visualize Feature Importance)
top_n = 21 # 상위 21개만 표시
plt.figure(figsize=(10, 8), dpi=1000)
# 상위 21개 변수 중요도 시각화
importances.sort_values().tail(top_n).plot.barh(color='skyblue')
plt.title(f'Feature Importance (MDI) for Divorce Prediction (Top {top_n} features)')
plt.xlabel('Mean Decrease Impurity (MDI)')
plt.ylabel('Feature')
plt.grid(axis='x', linestyle='--')

# 파일 저장 명령 제거 및 화면 출력만 유지
plt.show()

"""# **시각화 - 한결**

# **1. 워드 클라우드**
"""

# 1. Java 설치 (KoNLPy용)
!apt-get update -qq
!apt-get install -y openjdk-11-jdk -qq > /dev/null

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

# 2. KoNLPy 및 JPype1 설치
!pip install konlpy JPype1 -qq

# 3. 한글 폰트 설치 (워드클라우드용)
!apt-get install -y fonts-nanum -qq > /dev/null
!fc-cache -fv  # 폰트 캐시 갱신

# 4. 폰트 매니저 리빌드
import matplotlib.font_manager as fm

# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from wordcloud import WordCloud

# 설문 문항 리스트
sentences = [
    "아내와 나는 신뢰에 대한 가치를 가진다.",
    "아내와 저는 결혼 생활에 대한 생각이 비슷하다.",
    "무슨 일이 일어나고 있는지 알기도 전에 우리는 막 싸움을 시작하고 있지 않는다.",
    "언젠가 미래에 뒤돌아볼 때 아내와 나는 서로 조화를 이루고 있다고 생각한다.",
    "우리는 삶에서 행복해지는 것에 대해 아내와 일치하는 견해를 공유한다.",
    "나는 아내와 함께 여행하는 것을 즐긴다.",
    "아내와 나는 결혼 생활에서 역할이 어떠해야 하는지에 대한 생각이 비슷하다.",
    "나는 아내의 기본적인 걱정거리를 알고 있다.",
    "나는 아내의 내면 세계를 알고 있다.",
    "우리는 사랑이 어떠해야 하는지에 대해 아내와 의견이 일치한다.",
    "아내와 함께 살고 싶다는 꿈은 비슷하고 조화롭다.",
    "우리가 사람(자녀, 친구 등)에 대해 갖는 목표는 대부분 같다.",
    "싸움은 주 갑자기 일어나지 않는다.",
    "나는 우리가 논쟁할 때 굴욕감을 줄 수 없다.",
    "아내와 나는 개인의 자유 측면에서 비슷한 가치관을 가지고 있다.",
    "나는 아내와 함께 휴가를 즐긴다.",
    "나는 아내의 친구들과 그들의 사회적 관계를 인지하고 있다.",
    "나는 아내가 현재 스트레스를 받는 원인이 무엇인지 인지하고 있다.",
    "내가 아내와 무언가에 대해 이야기할 때, 내 차분함은 유지된다.",
    "나는 아내의 희망과 바람을 인지하고 있다.",
    "내 아내와의 논쟁은 차분하다."
]

# 텍스트 결합
text = " ".join(sentences)

# 형태소 분석 및 명사 추출
okt = Okt()
nouns = okt.nouns(text)


# 글자 수 2개 이상인 단어만 추출 (불용어 리스트 제거)
filtered_nouns = [word for word in nouns if len(word) >= 2]

stopwords = [
    "대해", "대한", "무엇", "무슨", "가지", "같다", "있다", "하다", "되다", "에서", "에게", "으로",
    "그리고", "이다", "아니다", "내", "저", "나", "등", "때", "일", "위해", "점", "안", "수","아내", "결혼", "우리"
]

# 불용어 제거 및 글자 수 2개 이상 필터링을 결합하여 적용
filtered_nouns = [
    word for word in nouns
    if word not in stopwords and len(word) >= 2
]

# 단어 연결
joined_text = " ".join(filtered_nouns)

# 색상 함수 (큰 단어는 진한 파랑, 작은 단어는 하늘색)
def blue_gradient_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    # font_size가 클수록 진한 파랑 (hsl의 밝기값 낮춤)
    # 글자 크기에 따라 밝기(lightness)를 90%에서 25%까지 조절
    lightness = max(25, 90 - font_size / 2)
    return f"hsl(210, 90%, {lightness}%)"

# 워드클라우드 생성
# font_path는 시스템 환경에 따라 변경될 수 있습니다.
wordcloud = WordCloud(
    font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
    background_color='white',
    width=1600,
    height=1000,
    color_func=blue_gradient_color_func,
    prefer_horizontal=0.95,
    max_words=150,
    min_font_size=10,
    max_font_size=250,
    collocations=False,
    relative_scaling=0.15,
    scale=3,
    margin=5
).generate(joined_text)

# 시각화
plt.figure(figsize=(14, 9),dpi=1000)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

# 정제된 단어 예시 출력 (참고용)
print("--- 정제된 단어 목록 예시 (글자 수 2개 이상) ---")
print(filtered_nouns[:100])

"""# **🥇 Top 5 핵심 키워드 (가장 중요한 요소)**
1. 논쟁
2. 신뢰
3. 생각
4. 싸움
5. 생활

# **2. 히트맵**
"""

!sudo apt-get install -y fonts-nanum > /dev/null
!sudo fc-cache -fv
!rm -rf ~/.cache/matplotlib

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
import numpy as np

# Matplotlib 한글 폰트 설정
try:
    plt.rc('font', family='NanumGothic')
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['axes.unicode_minus'] = False


# ✅ 설문 문항 매핑
QUESTION_MAP = {
    'Atr8': '나는 아내와 함께 휴가를 즐긴다.',
    'Atr9': '나는 아내와 함께 여행하는 것을 즐긴다.',
    'Atr11': '언젠가 미래에 뒤돌아볼 때 아내와 나는 서로 조화를 이루고 있다고 생각한다.',
    'Atr12': '아내와 나는 개인의 자유 측면에서 비슷한 가치관을 가지고 있다.',
    'Atr14': '우리가 사람(자녀, 친구 등)에 대해 갖는 목표는 대부분 같다.',
    'Atr15': '아내와 함께 살고 싶다는 꿈은 비슷하고 조화롭다.',
    'Atr16': '우리는 사랑이 어떠해야 하는지에 대해 아내와 의견이 일치한다.',
    'Atr17': '우리는 삶에서 행복해지는 것에 대해 아내와 같은 견해를 공유한다.',
    'Atr18': '아내와 저는 결혼 생활이 어떠해야 하는지에 대한 생각이 비슷한다.',
    'Atr19': '아내와 저는 결혼 생활에서 역할이 어떠해야 하는지에 대한 생각이 비슷하다.',
    'Atr20': '아내와 저는 신뢰에 대한 가치가 비슷하다.',
    'Atr25': '저는 아내의 내면 세계를 알고 있다.',
    'Atr26': '아내의 기본적인 걱정거리를 알고 있다.',
    'Atr27': '저는 아내가 현재 스트레스를 받는 원인이 무엇인지 알고 있다.',
    'Atr28': '나는 아내의 희망과 바람을 안다.',
    'Atr30': '나는 아내의 친구들과 그들의 사회적 관계를 안다.',
    'Atr36': '나는 우리가 논쟁할 때 굴욕감을 줄 수 없다.',
    'Atr37': '내 아내와의 논쟁은 차분하다.',
    'Atr39': '싸움이 갑자기 일어나지 않는다.',
    'Atr40': '나는 무슨 일이 일어나고 있는지 알기도 전에 우리는 싸움을 시작하지 않는다.',
    'Atr41': '내가 아내와 무언가에 대해 이야기할 때, 내 차분함은 유지된다.',
}

heatmap_features = list(QUESTION_MAP.keys())

print("'divorce_revised_selected_upper.xlsx' 파일을 업로드하세요.")
uploaded = files.upload()

df = pd.read_excel('/content/divorce_revised_selected_upper.xlsx')

valid_features = [col for col in heatmap_features if col in df.columns]
if not valid_features:
    raise SystemExit("⚠ 데이터셋에 해당 변수 없음")

corr_matrix = df[valid_features].corr()

filtered_map = {k: v for k, v in QUESTION_MAP.items() if k in valid_features}
corr_matrix.index = corr_matrix.index.map(filtered_map)
corr_matrix.columns = corr_matrix.columns.map(filtered_map)

# clustermap 생성 (colorbar 기본 위치로 만든 뒤 이동)
g = sns.clustermap(
    corr_matrix,
    cmap='coolwarm',
    annot=False,
    linewidths=.4,
    figsize=(15, 15),
    row_cluster=False,
    col_cluster=False,
)

# 히트맵 위치 가져오기
heatmap_pos = g.ax_heatmap.get_position()

# 컬러바 위치
cbar_width = 0.015  # 컬러바 두께
spacing = 0.04     # 히트맵과 컬러바 간 최소 간격

g.cax.set_position([
    heatmap_pos.x0 - cbar_width - spacing,
    heatmap_pos.y0,
    cbar_width,
    heatmap_pos.height
])

# 제목 위치
g.fig.text(
    (heatmap_pos.x0 + heatmap_pos.x1) / 2,
    heatmap_pos.y1 + 0.02,
    "설문 문항 간 상관관계 클러스터 히트맵",
    ha='center',
    va='bottom',
    fontsize=24,
    fontweight='bold'
)

plt.show()

"""# **강한 양의 상관관계 분석**
1. 유사성 및 조화:
  - Atr15(함께 살고 싶은 꿈), Atr17(삶의 행복 견해 공유), Atr11(미래에 조화를 이룸), Atr12(개인의 자유 가치관 유사)
  - 미래에 대한 공동의 비전과 핵심 가치관의 유사성
2. 소통 및 이해
  - Atr25(아내의 내면 세계), Atr28(아내의 희망과 바람), Atr27(아내의 스트레스 원인), Atr26(아내의 기본적인 걱정거리)
  - 배우자의 내면적인 감정, 희망, 그리고 어려움(스트레스)을 이해하는 심층적인 요소 문항
3. 차분한 논쟁
  - Atr37(논쟁은 차분하다), Atr41(이야기할 때 차분함 유지), Atr39(싸움이 갑자기 일어나지 않음)
  - 논쟁과 갈등 상황을 차분하고 예측 가능하게 관리하는 능력
  - 갈등 회피가 아니라 갈등 관리 능력이 만족도를 높이는 요소

# **강한 음의 상관관계**
- '나는 무슨 일이 일어나고 있는지 알기도 전에 우리는 싸움을 시작하지 않는다.'<->'언젠가 미래에 뒤돌아볼 때 아내와 나는 서로 조화를 이루고 있다고 생각한다.'
  -> 싸움이 예측 불가능해질수록 역설적으로 장기적 조화가 있다고 응답할 가능성 증가
- '나는 무슨 일이 일어나고 있는지 알기도 전에 우리는 싸움을 시작하지 않는다.' <-> '아내와 저는 결혼 생활에서 역할이 어떠해야 하는지에 대한 생각이 비슷하다.'
  -> 싸움이 예측 불가능할수록 결혼 역할에 대한 생각이 비슷해짐
- '나는 무슨 일이 일어나고 있는지 알기도 전에 우리는 싸움을 시작하지 않는다.' <-> '우리는 삶에서 행복해지는 것에 대해 아내와 같은 견해를 공유한다.'
  -> 싸움이 예측 불가능해질수록 행복에 대한 비슷한 견해를 가짐

# **3. 막대 그래프**
"""

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files
import os
import glob

# divorce_revised_selected_upper.xlsx 업로드
print(" 'divorce_revised_selected_upper.xlsx' 파일을 업로드하세요.")
uploaded = files.upload() # 업로드 창이 뜸

# 파일 불러오기
df = pd.read_excel('/content/divorce_revised_selected_upper.xlsx')

# 'Class'를 제외한 모든 변수(컬럼) 식별
feature_cols = [col for col in df.columns if col != 'Class']

# 'Class' (결혼 상태) 별 평균 응답 점수 계산
# Class=0: 기혼자 (Married), Class=1: 이혼자 (Divorced)
avg_scores = df.groupby('Class')[feature_cols].mean()
avg_scores.columns = feature_cols

# 각 변수(컬럼)별 막대 그래프 생성 및 결과창에 표시
for col in feature_cols:
    # 해당 변수의 기혼자와 이혼자 평균 점수 데이터를 추출
    data_to_plot = avg_scores[col]

    # Matplotlib Figure 및 Axes 생성
    fig, ax = plt.subplots(figsize=(7, 5), dpi=1000)

    class_labels = ['Married (Class=0)', 'Divorced (Class=1)']
    avg_values = data_to_plot.values

    # 막대 그래프 생성
    bars = ax.bar(class_labels, avg_values, color=['#c6dbef', '#08306b'])

    # 제목 및 축 라벨 설정 (모두 영어로 설정)
    ax.set_title(f'Average Response Score for Variable {col}', fontsize=14, fontweight='bold')
    ax.set_xlabel('Marital Status', fontsize=12)
    ax.set_ylabel('Average Score', fontsize=12)

    # y축 시작점을 0으로 설정하여 비율을 명확히 보여줍니다.
    if max(avg_values) > 0:
      ax.set_ylim(0, max(avg_values) * 1.2)
    else:
      ax.set_ylim(0, 1)

    # 각 막대 위에 평균 응답 점수 값 표시
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}',
                ha='center', va='bottom', fontsize=11, fontweight='semibold')

    # 그리드를 추가하여 가독성 향상
    ax.grid(axis='y', linestyle='--', alpha=0.7)

    # 그래프 요소가 잘리지 않도록 조정
    plt.tight_layout()
    plt.figure(dpi=1000)
    plt.show()

"""# **4. 바이올린 플랏**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # Violin Plot 시각화를 위해 seaborn 사용
from google.colab import files
import numpy as np

# 폰트 설정
plt.rcParams['axes.unicode_minus'] = False

# 변수 요약 정의
QUESTION_MAP = {
    'Atr8': 'Enjoy Holiday', 'Atr9': 'Enjoy Travel', 'Atr11': 'Long-term Harmony',
    'Atr12': 'Similar Values', 'Atr14': 'Similar Goals', 'Atr15': 'Dream Harmony',
    'Atr16': 'Love Definition', 'Atr17': 'Happiness View', 'Atr18': 'Marriage Views',
    'Atr19': 'Role Similarity', 'Atr20': 'Trust Values', 'Atr25': 'Know Inner World',
    'Atr26': 'Know Concerns', 'Atr27': 'Know Stressors', 'Atr28': 'Know Hopes',
    'Atr30': 'Know Social', 'Atr36': 'No Humiliation', 'Atr37': 'Calm Argument',
    'Atr39': 'Predictable Fight', 'Atr40': 'Fight Awareness', 'Atr41': 'Maintain Calm',
}
all_features = list(QUESTION_MAP.keys())



# 파일 업로드
# divorce_revised_selected_upper.xlsx 업로드
print(" 'divorce_revised_selected_upper.xlsx' 파일을 업로드하세요.")
uploaded = files.upload() # 업로드 창이 뜸

# 파일 불러오기
df = pd.read_excel('divorce_revised_selected_upper.xlsx')


# 시각화: 바이올린 플롯 (클래스별 21개 개별 그림)

print("\nVisualization: Generating 21 individual Violin Plots for score distribution by Marital Status.")

# 플롯 생성을 위한 데이터프레임 준비
valid_features = [col for col in all_features if col in df.columns]
plot_df = df[valid_features + ['Class']].copy()

# 클래스 라벨링 및 타입 변환
plot_df['Class'] = plot_df['Class'].astype('category').replace(
    {0: 'Married (0)', 1: 'Divorced (1)'}
)

# 21개 모든 변수를 순회하며 개별 바이올린 플롯 생성
for col in valid_features:
    # 피규어(Figure) 및 축(Axes) 생성
    fig, ax = plt.subplots(figsize=(7, 5))

    # 플롯 제목에 QUESTION_MAP을 사용하며, 없을 경우 컬럼 이름으로 대체
    title_label = QUESTION_MAP.get(col, col)

    # 바이올린 플롯 생성: X축=클래스, Y축=AtrX 점수
    # split=True: 두 그룹의 분포를 하나의 바이올린에 대칭적으로 보여주어 비교 용이
    # inner='quartile': 사분위수(중앙값, 25%, 75%)를 표시
    sns.violinplot(
        x='Class',
        y=col,
        data=plot_df,
        ax=ax,
        palette={'Married (0)': '#c6dbef', 'Divorced (1)': '#08306b'}, # 파스텔 톤 색상으로 변경
        inner='quartile',
        linewidth=1.5
    )

    # 각 그룹의 평균 선 계산 및 플롯 (선택 사항: 명확성을 위해)
    mean_scores = plot_df.groupby('Class')[col].mean()

    # 평균점 (점) 플롯
    ax.scatter(x=[0, 1], y=[mean_scores['Married (0)'], mean_scores['Divorced (1)']],
               color='black', zorder=3, s=80, label='Mean')

    # 제목 및 축 라벨 설정
    ax.set_title(f'Violin Plot: {title_label} ({col})', fontsize=14, fontweight='bold')
    ax.set_xlabel('Marital Status (Class)', fontsize=12)
    ax.set_ylabel('Response Score', fontsize=12)

    # Y축이 설문 점수 범위인 0부터 4를 포함하도록 설정
    ax.set_ylim(-0.5, 4.5)
    ax.grid(axis='y', linestyle='--', alpha=0.7)

    # 평균 점수 주석 추가
    ax.text(0, mean_scores['Married (0)'] - 0.4, f'Avg: {mean_scores["Married (0)"]:.2f}',
            ha='center', color='black', fontweight='bold', fontsize=10)
    ax.text(1, mean_scores['Divorced (1)'] - 0.4, f'Avg: {mean_scores["Divorced (1)"]:.2f}',
            ha='center', color='black', fontweight='bold', fontsize=10)

    plt.tight_layout()
    plt.figure(dpi=1000)
    plt.show()

"""# **전체 분포 분석 개요**
1. 극단적인 양극화: 응답 점수가 대체로 0점 또는 4점 근처에 압도적으로 집중됨.
2. 명확한 분리: 기혼 그룹과 이혼 그룹의 분포가 완전 분리됨.
3. 역방향 패턴: 기혼 그룹의 평균 점수가 높은 변수이면, 이혼 그룹의 평균 점수는 대체로 낮음.

# **패턴 1: 기혼 그룹이 높은 점수를 보이는 항목 (긍정적 관계 특성)**
1. 갈등 인식(40) -> 기혼 평균 3.79, 이혼 평균 0.43
2. 굴욕 주지 않음(36) -> 기혼 평균 3.63, 이혼 평균 0.45
3. 침착성 유지(41) -> 기혼 평균 3.53, 이혼 평균 0.46

# **패턴 2: 혼 그룹이 높은 점수를 보이는 항목 (긍정적 관계 특성)**
1. 역할 유사성(19) -> 기혼 평균 0.14, 이혼 평균 3.18
2. 장기 조화(11) -> 기혼 평균 0.2, 이혼 평균 3.18
3. 신뢰 가치(20) -> 기혼 평균 0.14, 이혼 평균 2.95

#**결론**
- 갈등을 겪을 때의 태도가 결혼 상태를 예측하는 데 있어 가장 강력한 분리력을 보여주는 논리적 근거.
- 생각, 가치관의 조화가 아무리 높을지라도 갈등을 겪는 태도가 무너지면 이혼에 이르기 쉬움.

# **5. 모자이크 플랏**
"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic
import matplotlib.font_manager as fm
import os
from google.colab import files

# divorce_revised_selected_upper.xlsx 업로드
print(" 'divorce_revised_selected_upper.xlsx' 파일을 업로드하세요.")
uploaded = files.upload() # 업로드 창이 뜸

# 파일 불러오기
df = pd.read_excel('divorce_revised_selected_upper.xlsx')

# 모든 컬럼을 범주형(Category)으로 변환
for col in df.columns:
    df[col] = df[col].astype('category')

# 'Class' 컬럼을 제외한 속성 목록 정의
attribute_cols = df.columns.drop('Class').tolist()

#  21개 모자이크 플랏 생성 및 출력

for col in attribute_cols:
    # 플랏마다 새로운 Figure 생성
    fig, ax = plt.subplots(figsize=(8, 6))

    # 모자이크 플랏 생성
    mosaic(df,
           index=['Class', col],
           ax=ax,
           title='',
           labelizer=lambda k: '',
           # Matplotlib 경고 제거를 위한 최신 컬러맵 문법 적용
           properties=lambda k: {'color': plt.colormaps['Blues'](int(k[1])/4)}
          )

    # 축 레이블 설정 (영문 유지)
    ax.set_xlabel('Class (0: Married, 1: Divorced)')
    ax.set_ylabel(f'{col} Value (0 to 4)')


    plt.show()

"""# **핵심 차별 요소: '갈등 및 논쟁 시의 태도'**
- 평균 차이(기혼자 평균 - 이혼자 평균)가 가장 크게 나타난 상위 5개 변수는 모두 배우자와의 갈등 상황에서 감정을 조절하고 존중하는 능력과 관련되어 있음
- 이 변수들에서 기혼자 그룹의 평균 점수(4점 만점 기준)가 압도적으로 높음
- 이는 기혼자들이 갈등 상황을 매우 긍정적이고 건설적으로 관리한다는 것을 의미

# **평균 차이가 가장 크게 나타난 상위 5개 변수(기혼자가 위)**
1. 나는 무슨 일이 일어나고 있는지 알기도 전에 우리는 싸움을 시작하지 않는다.(40)
2. 나는 우리가 논쟁할 때 굴욕감을 줄 수 없다.(36)
3. 내가 아내와 무언가에 대해 이야기할 때, 내 차분함은 유지된다.(41)
4. 싸움이 갑자기 일어나지 않는다.(39)
5. 내 아내와의 논쟁은 차분하다. (37)

# **평균 차이가 가장 크게 나타난 상위 5개 변수(이혼자가 위)**
1. 결혼 생활에서 역할에 대한 생각이 비슷하다.(19)
2. 언젠가 뒤돌아볼 때 서로 조화를 이루고 있다고 생각한다.(11)
3. 삶에서 행복해지는 것에 대해 아내와 같은 견해를 공유한다.(17)
4. 아내의 내면 세계를 알고 있다.(25)
5. 아내의 친구들과 그들의 사회적 관계를 안다.(30)

# **XAI 기법 적용**
- 11/12(수) ~

## 1. Decision Tree - 유림
"""

# ============================================
# 0. 라이브러리 설치
# ============================================
!pip install openpyxl
# ============================================
# 1. 기본 라이브러리 불러오기
# ============================================
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
# ============================================
# 2. 데이터 불러오기
#    (역코딩/삭제까지 끝난 최종 파일 사용)
# ============================================
df = pd.read_excel("/content/divorce_revised_selected.xlsx")

y = df["Class"]
X = df.drop(columns=["Class"])

print("X shape:", X.shape)
# ============================================
# 3. 랜덤포레스트로 상위 21개 변수 선택
# ============================================
rf = RandomForestClassifier(n_estimators=128, random_state=42)
rf.fit(X, y)

importances = pd.Series(rf.feature_importances_, index=X.columns)

top_n = 21
top_features = importances.sort_values(ascending=False).head(top_n).index.tolist()

# 상위 21개 변수만 사용
X_top = X[top_features]

# ============================================
# 4. 상위 21개 변수로 Decision Tree 학습
#    (시각화를 위해 너무 복잡하지 않게 max_depth 제한)
# ============================================
dt = DecisionTreeClassifier(
    criterion="gini",
    max_depth=4,         # 트리 깊이 (필요하면 3, 5 등으로 조절)
    min_samples_leaf=5,  # 너무 가지가 잘게 쪼개지지 않도록
    random_state=42
)
dt.fit(X_top, y)

print("\n✅ Decision Tree 학습 완료")

# ============================================
# 5. 결정트리 시각화
# ============================================
plt.figure(figsize=(24, 12))  # 슬라이드/보고서용이면 크게
plot_tree(
    dt,
    feature_names=top_features,      # 상위 21개 변수 이름
    class_names=["Not divorce", "Divorce"],  # Class 0, 1 이름
    filled=True,                    # 노드 색으로 클래스 비율 표시
    rounded=True,                   # 둥근 박스
    fontsize=9
)
plt.title("Decision Tree using Top 21 Features", fontsize=16)
plt.show()

"""### 결과 해석
--------------------------------
1. 결정 트리 전체 해석
- 상위 21개 변수 중에서
Atr18 → Atr20 순으로 가장 중요한 분기 기준을 찾아냄
- 이 두 문항이 이혼 여부를 가장 잘 구분하는 기준이라고 모델이 판단

2. 루트 노드 (맨 위)
- 데이터 전체(170명) 중 이혼 아님(0) 86명, 이혼(1) 84명으로 거의 반반임  -> gini=0.5 는 혼합도가 높다는 뜻

=> Atr18의 응답이 낮은가(≤1.5) vs 높은가(>1.5)에 따라 이혼 여부가 크게 갈림

3. 왼쪽 가지 (Atr18 ≤ 1.5인 사람들)
- Atr18 점수가 낮은 사람 89명 중
86명은 이혼이 아님, 3명만 이혼.거의 전부 “이혼 아님”으로 판단됨.
- 이 그룹 안에서도 Atr20 값이 0.5 이하인 경우 대부분 결혼 유지 쪽으로 분류됨.

=> Atr18과 Atr20이 둘 다 낮은 사람은 거의 확실하게 결혼 유지.

4. 오른쪽 가지 (Atr18 > 1.5인 사람들)

- Atr18이 높은 사람은 모두(81명) 이혼.
- gini=0.0은 완벽히 한쪽으로만 분류된다는 뜻이야.
- Atr18 문항에서 점수가 2 이상이면 이혼 확률 100%.

=> Atr18이 모델에서 ‘결정적 이혼 신호’ 역할을 함

----------------------------

| 분기 순서 | 조건                            | 결과             | 해석                   |
| ----- | ----------------------------- | -------------- | -------------------- |
| ①     | `Atr18 > 1.5`                 | → Divorce      | 이 문항 점수가 높으면 무조건 이혼  |
| ②     | `Atr18 ≤ 1.5`                 | → Atr20 확인     | 이 문항 점수가 낮으면 대부분 안정적 |
| ③     | `Atr18 ≤ 1.5` & `Atr20 ≤ 0.5` | → Not divorce  | 완전히 안정적 관계           |
| ④     | `Atr18 ≤ 1.5` & `Atr20 > 0.5` | → 약간 혼합(조금 위험) | 일부만 이혼, 대부분 안정       |



--------------

5. 결론

- Atr18: 가장 중요한 기준
-> 이 문항 점수가 높으면 거의 이혼

- Atr20: 보조 기준
-> Atr18이 낮은 사람 중에서도, Atr20 점수가 높으면 약간 위험

# 2. SAHP summary plot - 유림
- 11/17(월)
"""

# ==========================================================
# Divorce 설문 데이터: 상위 21개 문항에 대한 SHAP summary plot
# ==========================================================

# 0. 필요한 패키지 설치 (처음 한 번만 실행)
!pip install shap openpyxl

# 1. 라이브러리 불러오기
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
import shap

# ----------------------------------------------------------
# 2. 데이터 불러오기
# ----------------------------------------------------------
# divorce_revised_selected.xlsx : 역코딩 및 불필요 문항 삭제가 끝난 최종 데이터
df = pd.read_excel("/content/divorce_revised_selected.xlsx")

# Class : 타깃(이혼 여부), 나머지 Atr* : 설문 문항 점수
y = df["Class"]
X = df.drop(columns=["Class"])

print("✅ 데이터 크기:", X.shape)  # (샘플 수, 변수 수)
print("✅ 예시 컬럼:", list(X.columns[:10]))

# ----------------------------------------------------------
# 3. 전체 변수를 사용해 RandomForest 학습
#    → MDI(불순도 감소량) 기준 상위 21개 문항 선택
# ----------------------------------------------------------
rf_all = RandomForestClassifier(
    n_estimators=128,
    random_state=42
)
rf_all.fit(X, y)

# feature_importances_ : MDI 방식 변수 중요도
importances = pd.Series(rf_all.feature_importances_, index=X.columns)

top_n = 21
top_features = importances.sort_values(ascending=False).head(top_n).index.tolist()

print(f"\n🏅 MDI 기준 상위 {top_n}개 문항:")
for i, f in enumerate(top_features, 1):
    print(f"{i:2d}. {f}")

# 상위 21개 문항만 남긴 입력 데이터
X_top = X[top_features]

# ----------------------------------------------------------
# 4. 상위 21개 문항으로 다시 RandomForest 학습
#    (이 모델을 기준으로 SHAP 계산)
# ----------------------------------------------------------
rf_top = RandomForestClassifier(
    n_estimators=128,
    random_state=42
)
rf_top.fit(X_top, y)
print("\n✅ 상위 21개 문항으로 RandomForest 재학습 완료")

# ----------------------------------------------------------
# 5. SHAP 값 계산 (클래스 1 기준)
# ----------------------------------------------------------
explainer = shap.TreeExplainer(rf_top)
shap_values = explainer.shap_values(X_top)

print("shap_values 타입:", type(shap_values))

# shap 버전에 따라 출력 형태가 다를 수 있으므로 분기 처리
if isinstance(shap_values, list):
    # 리스트인 경우: [class0, class1, ...] 형태
    print("클래스 수:", len(shap_values))
    # 이진분류이므로 보통 '이혼'에 해당하는 class 1 사용
    shap_values_to_plot = shap_values[1]
else:
    # 배열인 경우: (샘플 수, 특성 수) 또는 (샘플 수, 특성 수, 클래스 수)
    print("shap_values shape:", shap_values.shape)
    if shap_values.ndim == 3:
        # (n_samples, n_features, n_classes) 형태라면
        n_classes = shap_values.shape[2]
        class_idx = 1 if n_classes > 1 else 0
        shap_values_to_plot = shap_values[:, :, class_idx]
        print("→ 클래스", class_idx, "기준 SHAP 사용, shape:", shap_values_to_plot.shape)
    else:
        shap_values_to_plot = shap_values

# ----------------------------------------------------------
# 6. SHAP Summary Plot (Top 21 features)
# ----------------------------------------------------------
plt.figure(figsize=(8, 10))
shap.summary_plot(
    shap_values_to_plot,  # 각 샘플×특성의 SHAP 값
    X_top,                # 상위 21개 문항만 포함된 데이터프레임
    plot_type="dot",
    max_display=top_n,
    show=False
)

plt.title(f"SHAP Summary Plot (Top {top_n} Features)", fontsize=14)
plt.tight_layout()
plt.show()

"""## 결과 해석

1. y축: 상위 21개 문항
- 위에 있을수록 전체적으로 더 중요한 문항
2. x축: SHAP value
- 0보다 오른쪽(+): 그 문항의 응답 때문에 이혼(class=1) 예측 확률이 증가한 방향
- 0보다 왼쪽(-): 그 문항의 응답 때문에 이혼이 아닐 (class=0) 확률이 증가한 방향
3. 색깔: 해당 문항의 실제 응답 값
- 파랑(Low): 점수가 낮은 응답
- 빨강(High): 점수가 높은 응답

4. 주요 변수 해석
- Atr40에서 핑크 점들이 왼쪽(–)으로 몰려 있으면
→ “Atr40 점수가 높을수록, 이혼이 아닐 쪽으로 예측이 간다”

- Atr18에서 핑크 점들이 오른쪽(+)이면
→ “Atr18 점수가 높을수록, 이혼 예측이 강화된다”

1위: Atr40
- 점수가 낮을수록 이혼 확률이 크게 증가하는 가장 중요한 변수

2위: Atr18
- 점수가 높을수록 이혼 확률을 꾸준히 증가시킴
- 전체적으로 점들이 균일하게 오른쪽으로 분포하여 '고점일수록 위험 증가' 패턴이 확실함

3위: Atr17
- 특정 범위 이상에서는 점수가 높아지면 이혼 위험이 급격히 증가하지만, 점수가 낮을 때는 거의 영향을 주지 않음
- (임계점이 있는 문항처럼 보임)

4위: Atr19
- 응답 점수가 높으면 이혼을 예측하는 방향으로 크게 기여하지만, 낮으면 오히려 '이혼하지 않은 방향'으로 기여

5위: Atr20
- 점수가 높을수록 이혼 쪽으로 약~중간 정도 기여

# 3. SAHP force plot - 유림
- 11/17(월)
"""

import shap
import numpy as np

# TreeExplainer와 SHAP 값 계산
explainer = shap.TreeExplainer(rf_top)
shap_values = explainer.shap_values(X_top)

# ✅ 클래스 1(이혼) 기준 SHAP 값만 추출
if isinstance(shap_values, list):
    # shap_values: [class0, class1] 형태
    shap_values_class1 = shap_values[1]
else:
    # shap_values: (n_samples, n_features) 또는 (n_samples, n_features, n_classes)
    if shap_values.ndim == 3:
        class_idx = 1 if shap_values.shape[2] > 1 else 0
        shap_values_class1 = shap_values[:, :, class_idx]
    else:
        shap_values_class1 = shap_values

print("shap_values_class1 shape:", shap_values_class1.shape)  # (샘플 수, 21)

# ✅ expected value도 클래스 1 기준으로 추출
ev = explainer.expected_value
if isinstance(ev, (list, np.ndarray)):
    expected_value_class1 = ev[1] if len(ev) > 1 else ev[0]
else:
    expected_value_class1 = ev

print("expected_value_class1:", expected_value_class1)

# 대표 샘플 3개 고르는 코드

import numpy as np

# 1️⃣ 각 샘플의 예측 확률 계산 (이혼=1 클래스 기준)
probs = rf_top.predict_proba(X_top)[:, 1]

# 2️⃣ 예측값이 가장 높은 사람, 낮은 사람, 중간 사람 인덱스 찾기
idx_high = np.argmax(probs)       # 이혼 확률 가장 높은 사람
idx_low = np.argmin(probs)        # 이혼 확률 가장 낮은 사람
idx_mid = np.argsort(np.abs(probs - 0.5))[0]  # 0.5 근처인 중간 사람

print("🔺 High case (이혼 위험 높음): idx =", idx_high, "확률 =", probs[idx_high])
print("🔹 Low case (이혼 위험 낮음): idx =", idx_low, "확률 =", probs[idx_low])
print("⚪ Mid case (중간): idx =", idx_mid, "확률 =", probs[idx_mid])

"""idx = 1 (이혼 확률 1.0)
- 대부분의 응답이 이혼 확률을 높이는 방향으로 작용함
-> 이혼 위험군 대표 사례
"""

# idx = 1 (이혼 확률 1.0)

# 자바스크립트 기반 SHAP 플롯 초기화 (Colab/노트북용)
shap.initjs()

# 예시로 첫 번째 샘플(idx = 0)을 선택
idx = 1   # 다른 샘플을 보고 싶으면 1, 2, ... 로 바꿔서 실행

# force plot (JS 버전, 상호작용 가능)
shap.force_plot(
    expected_value_class1,          # base value (평균 logit 또는 예측값)
    shap_values_class1[idx, :],     # 해당 샘플의 SHAP 값 (길이 21)
    X_top.iloc[idx, :],             # 해당 샘플의 특성 값
)

"""idx=1 간단한 결과 해석

- 이 사람은 Atr9, 11, 17, 18, 19, 20 같은 핵심 문항에서 점수가 꽤 높다(3~4점)
- 그리고 이 문항들의 SHAP 화살표가 모두 빨간색·오른쪽이기 때문에,
→ 모델은 “이 문항에서 이런 응답을 한 사람은 이혼일 가능성이 크다”고 학습한 상태
- Atr40 = 0 는 파란색으로 약간 왼쪽으로 밀고 있어서,
→ 이 문항의 응답은 오히려 이혼 가능성을 조금 줄이는 보호 요인으로 작용

idx = 84 (이혼 확률 0.0)
- 모델은 이 사람의 응답이 결혼 유지 쪽에 안정적으로 기여한다고 봄
-> 안정된 관계 대표 사례
"""

# idx = 84 (이혼 확률 0.0)

# 자바스크립트 기반 SHAP 플롯 초기화 (Colab/노트북용)
shap.initjs()

# 예시로 첫 번째 샘플(idx = 0)을 선택
idx = 84

# force plot (JS 버전, 상호작용 가능)
shap.force_plot(
    expected_value_class1,          # base value (평균 logit 또는 예측값)
    shap_values_class1[idx, :],     # 해당 샘플의 SHAP 값 (길이 21)
    X_top.iloc[idx, :],             # 해당 샘플의 특성 값
)

"""idx=84 간단한 결과 해석

- 이 사람은 중요 문항들에서 점수가 낮거나(0~1점), 특정 방향의 응답을 선택했고,
- 이 응답들이 모두 모델 입장에서 이혼 위험이 낮은 패턴으로 인식됨
- 특히 Atr18, 19, 20, 11, 9 같은 문항들은 high case에서 “위험 신호”였는데
여기서는 반대 방향의 응답(0점)에 가까워서,
→ 이혼 예측값을 많이 깎아내리는 파란 요인들로 작용

idx = 4 (이혼 확률 0.66)
- 빨강과 파랑이 섞여 있음
- 긍정, 부정 문항이 동시에 작용해 모델이 판단을 망설인 예시
-> 경계선(중간) 사례
"""

# idx = 4 (이혼 확률 0.66)

# 자바스크립트 기반 SHAP 플롯 초기화 (Colab/노트북용)
shap.initjs()

# 예시로 첫 번째 샘플(idx = 0)을 선택
idx = 4

# force plot (JS 버전, 상호작용 가능)
shap.force_plot(
    expected_value_class1,          # base value (평균 logit 또는 예측값)
    shap_values_class1[idx, :],     # 해당 샘플의 SHAP 값 (길이 21)
    X_top.iloc[idx, :],             # 해당 샘플의 특성 값
)

"""idx = 4 간단한 결과 해석

- idx=4 사람은 위험 신호와 보호 신호가 섞여 있기 때문에
- 평균보다 이혼 가능성은 높지만(0.66), high case처럼 극단적으로 위험하지는 않은 ‘경계선 사례’이다.
- 특정 문항에서는 문제를 보이지만, 다른 문항에서는 관계를 지켜주는 요소도 존재한다.

## 결과 해석

1. force plot이란?
-  한 사람(한 샘플)의 예측값이 '전체 평균(base value)'에서 '최종 예측값(output value)'으로 변하는 과정을 보여주는 원인 분석 그래프

2. 색의 의미
- 빨간색: 이 문항 때문에 이혼 예측값이 올라감
- 파란색: 이 문항 때문에 이혼 예측값이 내려감

3. 내용
- base value ≈ 0.49
→ 아무 정보도 모를 때, 평균적으로 이혼일 확률
- output value
→ 이 사람의 응답까지 반영했을 때 이혼일 확률
- 빨간 블록 (higher)
→ 이 문항의 응답이 이혼 쪽(Class=1) 으로 예측을 밀어 올리는 요인
- 파란 블록 (lower)
→ 이 문항의 응답이 이혼이 아닐 쪽(Class=0) 으로 예측을 끌어내리는 요인

＝＞ 이 force plot은 개인 단위의 원인 분석(local explanationn)을 해주는 그래프

→ summary plot은 전체적인 변수 영향력을 보여줌

→ force plot은 한 사람의 예측 결과를 만든 구체적 원인을 보여줌

# 4. SHAP Waterfall Plot - 문영신
"""

from sklearn.cluster import KMeans
import numpy as np

representatives = {}

for cls in [0, 1]:
    X_cls = X_top[y == cls]

    # KMeans 클러스터링 (3개 그룹)
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_cls)

    centers = kmeans.cluster_centers_
    labels = kmeans.labels_

    rep_idx = []

    for i in range(3):
        cluster_points = X_cls[labels == i]
        cluster_idx = cluster_points.index

        # centroid와 가장 가까운 샘플 찾기
        distances = np.linalg.norm(cluster_points.values - centers[i], axis=1)
        nearest_local = np.argmin(distances)

        rep_idx.append(int(cluster_idx[nearest_local]))  # 🔥 반드시 int로 변환

    representatives[cls] = rep_idx

print("\n📌 클래스별 대표 샘플 인덱스 (정수형으로 출력):")
for cls in representatives:
    print(f"Class {cls}: {representatives[cls]}")

# ================================================================
# PCA + 클래스별 KMeans 시각화 (대표 샘플은 별 표시)
# ================================================================
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ---- 1) PCA (2차원 축소) ----
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_top)

# PCA 결과를 데이터프레임화 (클래스 / 대표 여부 시각화 위해)
pca_df = pd.DataFrame({
    "PC1": X_pca[:, 0],
    "PC2": X_pca[:, 1],
    "Class": y.values
})

# ---- 2) 클래스별 시각화 ----
for cls in [0, 1]:
    X_cls_idx = pca_df[pca_df["Class"] == cls].index

    # 클래스 데이터만 선택
    X_cls_pca = X_pca[X_cls_idx]

    # KMeans 3클러스터
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_top.loc[X_cls_idx])
    labels_cls = kmeans.labels_

    # ---- 시각화 ----
    plt.figure(figsize=(7, 6), dpi = 500)
    scatter = plt.scatter(
        X_cls_pca[:, 0],
        X_cls_pca[:, 1],
        c=labels_cls,
        cmap='viridis',
        s=60,
        alpha=0.85
    )

    # 대표 샘플 표시
    reps = representatives[cls]   # 대표 샘플 인덱스 3개
    for ridx in reps:
        plt.scatter(
            pca_df.loc[ridx, "PC1"],
            pca_df.loc[ridx, "PC2"],
            s=200,
            marker='*',
            color='red',
            linewidths=1.3,
            label="Representative Sample" if ridx == reps[0] else ""
        )

    plt.title(f"Class {cls} - KMeans Clustering (PCA 2D)", fontsize=15)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.colorbar(scatter, label="Cluster Label")

    # 범례 한 번만 표시
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()

import matplotlib
matplotlib.rcParams['axes.unicode_minus'] = False

for cls in representatives:
    print(f"\n==============================")
    print(f"      CLASS {cls} 대표 샘플")
    print(f"==============================")

    for idx in representatives[cls]:
        print(f"\n📌 Waterfall Plot — Sample Index {idx}")

        expl = shap.Explanation(
            values = shap_values_to_plot[idx],
            base_values = explainer.expected_value[1],  # 클래스 1 기준
            data = X_top.loc[idx].values,
            feature_names = X_top.columns
        )

        shap.plots.waterfall(expl, max_display=21)
        plt.figure(dpi=1000)

"""# 5. SHAP decision plot - 유림"""

import numpy as np

# 이혼(클래스 1) 확률
probs = rf_top.predict_proba(X_top)[:, 1]

idx_high = np.argmax(probs)                          # 이혼 확률 가장 높은 사람
idx_low  = np.argmin(probs)                          # 이혼 확률 가장 낮은 사람
idx_mid  = np.argsort(np.abs(probs - 0.5))[0]        # 0.5에 가장 가까운 사람

print("🔺 High case :", idx_high, "prob =", probs[idx_high])
print("🔹 Low case  :", idx_low,  "prob =", probs[idx_low])
print("⚪ Mid case  :", idx_mid,  "prob =", probs[idx_mid])

sample_idx = [idx_high, idx_low, idx_mid]

# explainer.expected_value도 이진분류라면 [class0, class1] 형태일 수 있음
ev = explainer.expected_value
if isinstance(ev, (list, np.ndarray)):
    expected_value_class1 = ev[1] if len(ev) > 1 else ev[0]
else:
    expected_value_class1 = ev

print("expected_value_class1:", expected_value_class1)

import matplotlib.pyplot as plt
import shap
import numpy as np

# 1) 이혼 확률 기반으로 대표 샘플 3명 선택
probs = rf_top.predict_proba(X_top)[:, 1]

idx_high = np.argmax(probs)
idx_low  = np.argmin(probs)
idx_mid  = np.argsort(np.abs(probs - 0.5))[0]

sample_idx = [idx_high, idx_low, idx_mid]

# 2) SHAP expected value 정리
ev = explainer.expected_value
if isinstance(ev, (list, np.ndarray)):
    expected_value_class1 = ev[1] if len(ev) > 1 else ev[0]
else:
    expected_value_class1 = ev

# 3) feature 이름을 list로 바꿔 SHAP에 전달
feature_names = list(X_top.columns)

# 4) SHAP Decision Plot
shap.decision_plot(
    expected_value_class1,                # base value
    shap_values_to_plot[sample_idx, :],   # SHAP values for selected samples
    feature_names=feature_names,
    legend_labels=[
        f"High (idx={idx_high})",
        f"Low (idx={idx_low})",
        f"Mid (idx={idx_mid})"
    ],
    legend_location="upper right"
)